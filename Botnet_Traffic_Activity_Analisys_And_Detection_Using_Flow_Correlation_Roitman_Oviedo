{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data set downloaded rom: https://statweb.stanford.edu/~sabatti/data.html\n",
    "# Challenge description by the author:\n",
    "\n",
    "#Computer Network Traffic Data - A ~500K CSV with summary of some real network traffic data from the past. \n",
    "#The dataset has ~21K rows and covers 10 local workstation IPs over a three month period. \n",
    "#Half of these local IPs were compromised at some point during this period and became members of various botnets. \n",
    "#Can you discover when a compromise has occurred by a change in the pattern of communication?\n",
    "\n",
    "#Each row consists of four columns:\n",
    "#date: yyyy-mm-dd (from 2006-07-01 through 2006-09-30)\n",
    "#l_ipn: local IP (coded as an integer from 0-9)\n",
    "#r_asn: remote ASN (an integer which identifies the remote ISP)\n",
    "#f: flows (count of connnections for that day)\n",
    "#Reports of \"odd\" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier)\n",
    "#Date : IP\n",
    "#08-24 : 1\n",
    "#09-04 : 5\n",
    "#09-18 : 4\n",
    "#09-26 : 3 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My approach to tackle the challenge:\n",
    "#Botnet forensics usually employs all data possible from network traffic captures, to would allow the investigator to\n",
    "#achieve several levels of classification to help zero down to the infected computers, used ports, and used services during attacks.\n",
    "\n",
    "#Due to the limited data offered by the data set of posted challenge; and honoring the statement that \n",
    "#...\"Half of these local IPs were compromised at some point during this period and became members of various botnets.\"\n",
    "#I am willing to demonstrate that applying flow correlation may help me detect what computers were infested and some details about\n",
    "#the butnets attacks that took place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8074df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import io #This is for pandas.io.formats.style.Styler\n",
    "import requests\n",
    "import matplotlib.pyplot as plot #library for plotting of time series.\n",
    "import seaborn #library for correlation matrix.\n",
    "from pandas import to_datetime\n",
    "pandas.set_option('display.max_rows', 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pandas.read_csv('dataset1a.csv') #Reading data set downloaded from 'https://statweb.stanford.edu/~sabatti/data.html', name: 'Computer Network Traffic Data'.\n",
    "url=\"http://hci.stanford.edu/courses/cs448b/data/ipasn/cs448b_ipasn.csv\"\n",
    "s=requests.get(url).content\n",
    "df=pandas.read_csv(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3915637",
   "metadata": {},
   "source": [
    "# _1 Exploring / Understanding the Data Set's features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020aa234",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df.head(30)) #Exploring: inspecting data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01244628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Frame shape (rows,cols): {} '.format(df.shape)) #Exploring: Querying shape of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649122b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe() #Exploring: querying basic data set statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16569489",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df) #Exploring: confirming it's a dataFrame, not a list, array or series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #Exploring: checking data set feature sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bfe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring: printing a report with data facts.\n",
    "print('Time period data available (days) {:d}'.format(df['date'].nunique()))\n",
    "print('Number of Computers (ipn) {:d}'.format(df['l_ipn'].nunique()))\n",
    "print('Number of remote servers (ASN) {:d}'.format(df['r_asn'].nunique()))\n",
    "print('Minimum connections count per day {:d}'.format(df['f'].min()))\n",
    "print('Maximum connections count per day {:d}'.format(df['f'].max()))\n",
    "print('Missing or NAN values in dataset {:d}', df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a455a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "top=df['r_asn'].value_counts().nlargest(100) #Exploring: querying the top-most contacted servers and their total count.\n",
    "top_df=pandas.DataFrame(top)\n",
    "top_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a128c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generating: a variable for each computer to store their aggregate flows for use in the time series graphs.\n",
    "Computer1 = df.loc[df[\"l_ipn\"] == 0]\n",
    "Computer2 = df.loc[df[\"l_ipn\"] == 1]\n",
    "Computer3 = df.loc[df[\"l_ipn\"] == 2]\n",
    "Computer4 = df.loc[df[\"l_ipn\"] == 3]\n",
    "Computer5 = df.loc[df[\"l_ipn\"] == 4]\n",
    "Computer6 = df.loc[df[\"l_ipn\"] == 5]\n",
    "Computer7 = df.loc[df[\"l_ipn\"] == 6]\n",
    "Computer8 = df.loc[df[\"l_ipn\"] == 7]\n",
    "Computer9 = df.loc[df[\"l_ipn\"] == 8]\n",
    "Computer10 = df.loc[df[\"l_ipn\"] == 9]\n",
    "type(Computer1) #test\n",
    "Computer1.loc[Computer1['r_asn']==13462].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating daily flow aggregates of all computers...and then for each computer then querying basic stats (for the plots).\n",
    "daily_aggregate_flow = df.groupby(['date'])[['f']].sum() #Daily aggregate of all traffic\n",
    "daily_aggregate_flow.reset_index(inplace = True)\n",
    "\n",
    "daily_aggregate_flow1 = Computer1.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow1.reset_index(inplace = True)\n",
    "daily_aggregate_flow1 = daily_aggregate_flow1.rename(columns={'f':'flows_C#1'})\n",
    "\n",
    "daily_aggregate_flow2 = Computer2.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow2.reset_index(inplace = True)\n",
    "daily_aggregate_flow2 = daily_aggregate_flow2.rename(columns={'f':'flows_C#2'})\n",
    "\n",
    "daily_aggregate_flow3 = Computer3.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow3.reset_index(inplace = True)\n",
    "daily_aggregate_flow3 = daily_aggregate_flow3.rename(columns={'f':'flows_C#3'})\n",
    "\n",
    "daily_aggregate_flow4 = Computer4.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow4.reset_index(inplace = True)\n",
    "daily_aggregate_flow4 = daily_aggregate_flow4.rename(columns={'f':'flows_C#4'})\n",
    "\n",
    "daily_aggregate_flow5 = Computer5.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow5.reset_index(inplace = True)\n",
    "daily_aggregate_flow5 = daily_aggregate_flow5.rename(columns={'f':'flows_C#5'})\n",
    "\n",
    "daily_aggregate_flow6 = Computer6.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow6.reset_index(inplace = True)\n",
    "daily_aggregate_flow6 = daily_aggregate_flow6.rename(columns={'f':'flows_C#6'})\n",
    "\n",
    "daily_aggregate_flow7 = Computer7.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow7.reset_index(inplace = True)\n",
    "daily_aggregate_flow7 = daily_aggregate_flow7.rename(columns={'f':'flows_C#7'})\n",
    "\n",
    "daily_aggregate_flow8 = Computer8.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow8.reset_index(inplace = True)\n",
    "daily_aggregate_flow8 = daily_aggregate_flow8.rename(columns={'f':'flows_C#8'})\n",
    "\n",
    "daily_aggregate_flow9 = Computer9.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow9.reset_index(inplace = True)\n",
    "daily_aggregate_flow9 = daily_aggregate_flow9.rename(columns={'f':'flows_C#9'})\n",
    "\n",
    "daily_aggregate_flow10 = Computer10.groupby(['date'])[['f']].sum()\n",
    "daily_aggregate_flow10.reset_index(inplace = True)\n",
    "daily_aggregate_flow10 = daily_aggregate_flow10.rename(columns={'f':'flows_C#10'})\n",
    "\n",
    "#daily_aggregate_flow of all Computers\n",
    "daily_aggregate_flow['f'].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a daily mean of flows for use in the time series graph.\n",
    "daily_mean = round(daily_aggregate_flow['f'].mean(),6)\n",
    "\n",
    "#daily_mean1 = round(daily_aggregate_flow1['flows_C#1'].mean(),6)\n",
    "print(daily_mean1)\n",
    "#daily_mean2 = round(daily_aggregate_flow2['flows_C#2'].mean(),6)\n",
    "#daily_mean3 = round(daily_aggregate_flow3['flows_C#3'].mean(),6)\n",
    "#daily_mean4 = round(daily_aggregate_flow4['flows_C#4'].mean(),6)\n",
    "#daily_mean5 = round(daily_aggregate_flow5['flows_C#5'].mean(),6)\n",
    "#daily_mean6 = round(daily_aggregate_flow6['flows_C#6'].mean(),6)\n",
    "#daily_mean7 = round(daily_aggregate_flow7['flows_C#7'].mean(),6)\n",
    "#daily_mean8 = round(daily_aggregate_flow8['flows_C#8'].mean(),6)\n",
    "#daily_mean9 = round(daily_aggregate_flow9['flows_C#9'].mean(),6)\n",
    "#daily_mean10 = round(daily_aggregate_flow10['flows_C#10'].mean(),6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a dictionary with the info posted with the data set challenge about suspected attack dates.\n",
    "dictionary = {'2006-08-24':1,'2006-09-04':5,'2006-09-18':4,'2006-09-26':3}\n",
    "marked_anom = pandas.DataFrame.from_dict(dictionary,orient='index')\n",
    "marked_anom.reset_index(inplace = True)\n",
    "marked_anom.columns = ['date','l_ipn']\n",
    "print(marked_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56618ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring: checking for suspected attack on servers from suspected compromised computers on suspicious activity dates.\n",
    "#NOTE: found no repeats. Compputer flow correlation may not be strong on those days. Pattern?\n",
    "Suspicion1_Comp1= Computer1.loc[Computer1['date']=='2006-08-24']\n",
    "susp1=(Suspicion1_Comp1['r_asn'].value_counts()[:100].index.tolist())\n",
    "Suspicion2_Comp5= Computer5.loc[Computer5['date']=='2006-09-04']\n",
    "susp2=(Suspicion2_Comp5['r_asn'].value_counts()[:100].index.tolist())\n",
    "Suspicion3_Comp4= Computer4.loc[Computer4['date']=='2006-09-18']\n",
    "susp3=(Suspicion3_Comp4['r_asn'].value_counts()[:100].index.tolist())\n",
    "Suspicion4_Comp3= Computer3.loc[Computer3['date']=='2006-09-26']\n",
    "susp4=(Suspicion4_Comp3['r_asn'].value_counts()[:100].index.tolist())\n",
    "Suspicion5_Comp6= Computer6.loc[Computer6['date']=='2006-09-26']\n",
    "susp5=(Suspicion5_Comp6['r_asn'].value_counts()[:100].index.tolist())\n",
    "\n",
    "Suspicious_Comp_Activity = pandas.DataFrame.from_dict(\n",
    "    {'Susp1_Comp1':susp1,\n",
    "     'Susp2_Comp5':susp2,\n",
    "     'Susp3_Comp4':susp3,\n",
    "     'Susp4_Comp3':susp4,\n",
    "     'Susp5_Comp6':susp5}, orient='index')\n",
    "\n",
    "Suspicious_Comp_Activity.transpose().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0785a",
   "metadata": {},
   "source": [
    "# _2 Calculating and Visualizing Aggregate Flows (number of connections by each l_ipn computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of all Computers during the 92 day observation period.\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean Of All Connections')\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Daily Aggregate Flows Of All Computers',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50832127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#1 during the 92 day observation period.\n",
    "daily_mean1 = round(daily_aggregate_flow1['flows_C#1'].mean(),6)\n",
    "print (daily_mean1)\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean1, color='b', label = 'Mean Of Computer#1')\n",
    "plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow1['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#1 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#2 during the 92 day observation period.\n",
    "daily_mean2 = round(daily_aggregate_flow2['flows_C#2'].mean(),6)\n",
    "print (daily_mean2)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean2, color='b', label = 'Mean Of Computer#2')\n",
    "plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow2['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#2 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#3 during the 92 day observation period.\n",
    "daily_mean3 = round(daily_aggregate_flow3['flows_C#3'].mean(),6)\n",
    "print (daily_mean3)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean3, color='b', label = 'Mean OF Computer#3')\n",
    "plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow3['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#3 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9351d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#4 during the 92 day observation period.\n",
    "daily_mean4 = round(daily_aggregate_flow4['flows_C#4'].mean(),6)\n",
    "print (daily_mean4)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean4, color='b', label = 'Mean Of Computer#4')\n",
    "plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow4['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#4 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#5 during the 92 day observation period.\n",
    "daily_mean5 = round(daily_aggregate_flow5['flows_C#5'].mean(),6)\n",
    "print (daily_mean5)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean5, color='b', label = 'Mean Of Computer#5')\n",
    "plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow5['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#5 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#6 during the 92 day observation period.\n",
    "daily_mean_flow6 = round(daily_aggregate_flow6['flows_C#6'].mean(),6)\n",
    "print (daily_mean_flow6)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean_flow6, color='b', label = 'Mean Of Computer#6')\n",
    "plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow6['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#6 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620f0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#7 during the 92 day observation period.\n",
    "daily_mean7 = round(daily_aggregate_flow7['flows_C#7'].mean(),6)\n",
    "print (daily_mean7)\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean7, color='b', label = 'Mean Of Computer#7')\n",
    "plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow7['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#7 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47490b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#8 during the 92 day observation period.\n",
    "daily_mean8 = round(daily_aggregate_flow8['flows_C#8'].mean(),6)\n",
    "print (daily_mean8)\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean8, color='b', label = 'Mean Of Computer#8')\n",
    "plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow8['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#8 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#9 during the 92 day observation period.\n",
    "daily_mean9 = round(daily_aggregate_flow9['flows_C#9'].mean(),6)\n",
    "print (daily_mean9)\n",
    "\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean9, color='b', label = 'Mean Of Computer#9')\n",
    "plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow9['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#9 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Daily Aggregate Flows of Computer#10 during the 92 day observation period.\n",
    "daily_mean10 = round(daily_aggregate_flow10['flows_C#10'].mean(),6)\n",
    "print (daily_mean10)\n",
    "plot.figure(figsize=(20,5))\n",
    "plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'])\n",
    "[plot.axvline(x=_x, color='r' , label = 'Suspected Anomaly {}'.format(ip)) for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean10, color='b', label = 'Mean Of Computer#10')\n",
    "plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow10['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Computer#10 Daily Aggregate Flows',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'],color='lightsteelblue')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16169130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: report to corroborate daily aggregate flows of two computers with correlated flow(to verify plot).\n",
    "testComputerIndexed_a = Computer1.set_index('date')\n",
    "flow_testComputer_a = testComputerIndexed_a['f'].groupby('date').sum()\n",
    "flow_a = pandas.DataFrame(numpy.array(flow_testComputer_a), columns=['f'])\n",
    "flow_a.columns = ['fa']\n",
    "type(flow_a)\n",
    "\n",
    "\n",
    "testComputerIndexed_b = Computer3.set_index('date')\n",
    "flow_testComputer_b = testComputerIndexed_b['f'].groupby('date').sum()\n",
    "flow_b = pandas.DataFrame(numpy.array(flow_testComputer_b), columns=['f'])\n",
    "flow_b.columns = ['fb']\n",
    "flow_b\n",
    "\n",
    "flow_a['fb']=flow_b\n",
    "flow_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all flows of daily aggregate conenctions.\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'], color= 'r', label='C1')\n",
    "plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'], color= 'yellow', label='C2')\n",
    "plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'], color= 'black', label='C3')\n",
    "plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'], color= 'green', label='C4')\n",
    "plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'], color= 'springgreen', label='C5')\n",
    "plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'], color= 'cornflowerblue', label='C6')\n",
    "plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'], color= 'm', label='C7')\n",
    "plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'], color= 'darkviolet', label='C8')\n",
    "plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'], color= 'white', label='C9')\n",
    "plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'], color= 'aquamarine', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean')\n",
    "#plot.plot(daily_aggregate['date'],daily_aggregate['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Daily Aggregate of all flows during the 92 day observed period',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67661435",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting a correlation matrix of all computer's daily aggregate flows.\n",
    "corrMatrix = pandas.concat([daily_aggregate_flow1['flows_C#1'], daily_aggregate_flow2['flows_C#2'], daily_aggregate_flow3['flows_C#3'], daily_aggregate_flow4['flows_C#4'], daily_aggregate_flow5['flows_C#5'], daily_aggregate_flow6['flows_C#6'], daily_aggregate_flow7['flows_C#7'], daily_aggregate_flow8['flows_C#8'], daily_aggregate_flow9['flows_C#9'], daily_aggregate_flow10['flows_C#10']], axis=1).corr()\n",
    "plot.subplots(figsize=(10,10))\n",
    "seaborn.heatmap(corrMatrix.iloc[:, 0:10:], annot=True, linewidths=4)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b876c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Visualizing: activity of the flow correlated Computer#1 and Computer#3.\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'], color= 'r', label='C1')\n",
    "#plot.plot(daily_aggregate_flow2['date'],daily_aggregatee_flow2['flows_C#2'], color= 'yellow', label='C2')\n",
    "plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'], color= 'black', label='C3')\n",
    "#plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'], color= 'green', label='C4')\n",
    "#plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'], color= 'springgreen', label='C5')\n",
    "#plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'], color= 'midnightblue', label='C6')\n",
    "#plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'], color= 'm', label='C7')\n",
    "#plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'], color= 'darkviolet', label='C8')\n",
    "#plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'], color= 'firebrick', label='C9')\n",
    "#plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'], color= 'white', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean')\n",
    "#plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('#1 Correlated flows of Computer#1 and Computer#3',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Report: Most contacted servers by correlated Computer#1 and Computer#3.\n",
    "m1=sorted(Computer1['r_asn'].value_counts()[:30].index.tolist())\n",
    "m3=sorted(Computer3['r_asn'].value_counts()[:30].index.tolist())\n",
    "m1_3 = pandas.DataFrame(\n",
    "    {'m1': m1,\n",
    "     'm3': m3,})\n",
    "m1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Visualizing: activity of the flow correlated Computer#2 and Computer#5\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "#plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'], color= 'r', label='C1')\n",
    "plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'], color= 'yellow', label='C2')\n",
    "#plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'], color= 'black', label='C3')\n",
    "#plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'], color= 'green', label='C4')\n",
    "plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'], color= 'springgreen', label='C5')\n",
    "#plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'], color= 'midnightblue', label='C6')\n",
    "#plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'], color= 'm', label='C7')\n",
    "#plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'], color= 'darkviolet', label='C8')\n",
    "#plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'], color= 'firebrick', label='C9')\n",
    "#plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'], color= 'white', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean')\n",
    "#plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('#2 Correlated flows of Computer#2 and Computer#5',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Report: Most contacted servers by correlated Computer#2 and Computer#5\n",
    "m2=sorted(Computer2['r_asn'].value_counts()[:30].index.tolist())\n",
    "m5=sorted(Computer5['r_asn'].value_counts()[:30].index.tolist())\n",
    "m2_5 = pandas.DataFrame(\n",
    "    {'m2': m2,\n",
    "     'm5': m5,})\n",
    "m2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234769e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Visualizing: activity of the flow correlated Computer#3 and Computer#9.\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "#plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'], color= 'r', label='C1')\n",
    "#plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'], color= 'yellow', label='C2')\n",
    "plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'], color= 'black', label='C3')\n",
    "#plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'], color= 'green', label='C4')\n",
    "#plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'], color= 'springgreen', label='C5')\n",
    "#plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'], color= 'midnightblue', label='C6')\n",
    "#plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'], color= 'm', label='C7')\n",
    "#plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'], color= 'darkviolet', label='C8')\n",
    "plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'], color= 'firebrick', label='C9')\n",
    "#plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'], color= 'white', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean')\n",
    "#plot.plot(daily_aggregate['date'],daily_aggregate['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('#3 Correlated flows of Computer#3 and Computer#9',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Report: Most contacted servers by correlated Computer#3 and Computer#9.\n",
    "m3=sorted(Computer3['r_asn'].value_counts()[:30].index.tolist())\n",
    "m9=sorted(Computer9['r_asn'].value_counts()[:30].index.tolist())\n",
    "m3_9 = pandas.DataFrame(\n",
    "    {'m3': m3,\n",
    "     'm9': m9,})\n",
    "m3_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd59618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Visualizing: activity of the flow correlated Computer#4 and Computer#6 and Computer#7.\n",
    "plot.figure(figsize=(30,10))\n",
    "plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'])\n",
    "#plot.plot(daily_aggregate_flow1['date'],daily_aggregate_flow1['flows_C#1'], color= 'r', label='C1')\n",
    "#plot.plot(daily_aggregate_flow2['date'],daily_aggregate_flow2['flows_C#2'], color= 'yellow', label='C2')\n",
    "#plot.plot(daily_aggregate_flow3['date'],daily_aggregate_flow3['flows_C#3'], color= 'black', label='C3')\n",
    "plot.plot(daily_aggregate_flow4['date'],daily_aggregate_flow4['flows_C#4'], color= 'green', label='C4')\n",
    "#plot.plot(daily_aggregate_flow5['date'],daily_aggregate_flow5['flows_C#5'], color= 'springgreen', label='C5')\n",
    "plot.plot(daily_aggregate_flow6['date'],daily_aggregate_flow6['flows_C#6'], color= 'midnightblue', label='C6')\n",
    "plot.plot(daily_aggregate_flow7['date'],daily_aggregate_flow7['flows_C#7'], color= 'm', label='C7')\n",
    "#plot.plot(daily_aggregate_flow8['date'],daily_aggregate_flow8['flows_C#8'], color= 'darkviolet', label='C8')\n",
    "#plot.plot(daily_aggregate_flow9['date'],daily_aggregate_flow9['flows_C#9'], color= 'firebrick', label='C9')\n",
    "#plot.plot(daily_aggregate_flow10['date'],daily_aggregate_flow10['flows_C#10'], color= 'white', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.axhline(y= daily_mean, color='b', label = 'Mean')\n",
    "#plot.plot(daily_aggregate_flow['date'],daily_aggregate_flow['f'].rolling(5).mean(), color='darkorange', label = '5 days Moving Average')\n",
    "plot.xticks(daily_aggregate_flow['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('#4 Correlated flows of Computer#4 and Computer#6 and Computer#7',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_aggregate_flow['date'],daily_aggregate_flow['f'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08027019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Report: Most contacted servers by correlated computers#4.#5 and #6   \n",
    "m4=sorted(Computer4['r_asn'].value_counts()[:30].index.tolist())\n",
    "m6=sorted(Computer6['r_asn'].value_counts()[:30].index.tolist())\n",
    "m7=sorted(Computer7['r_asn'].value_counts()[:30].index.tolist())\n",
    "m4_7_6 = pandas.DataFrame(\n",
    "    {'m4': m4,\n",
    "     'm6': m7,\n",
    "     'm7': m6})\n",
    "m4_7_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ed6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Planning next steps:\n",
    "#    a) Evaluate correlations during the dates where attacks are suspected.\n",
    "#    b) Filter data to find most common r_asn(Remote Servers) connections within correlated relations.\n",
    "#    c) BTry building a Markov Transitions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate report of daily number of flows by day, by computer (data Desription).\n",
    "daily_aggreg_flows_df = df.set_index(['date','r_asn','l_ipn']).unstack('l_ipn') \n",
    "daily_aggreg_flows_df.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate report of daily number of flows by day, by computer.\n",
    "daily_aggreg_flows_df.describe().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating: variables to store queries of connections of correlated computers with some of the most contacted servers.\n",
    "#df.loc[(df[\"l_ipn\"] == 3) & (df[\"date\"] == '2006-09-06')] #Confirming there was no traffic from Computer5 after 2006-09-06.\n",
    "asn_3269_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 3269)]\n",
    "asn_3269_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 3269)]\n",
    "asn_3269_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 3269)]\n",
    "asn_3269 = df.loc[df['r_asn'] == 3269]\n",
    "\n",
    "asn_3320_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 3320)]\n",
    "asn_3320_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 3320)]\n",
    "asn_3320_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 3320)]\n",
    "asn_3320 = df.loc[df['r_asn'] == 3320]\n",
    "\n",
    "asn_4134_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4134)]\n",
    "asn_4134_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4134)]\n",
    "asn_4134_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4134)]\n",
    "asn_4134 = df.loc[df['r_asn'] == 4134]\n",
    "\n",
    "asn_4713_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4713)]\n",
    "asn_4713_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4713)]\n",
    "asn_4713_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4713)]\n",
    "asn_4713 = df.loc[df['r_asn'] == 4713]\n",
    "\n",
    "asn_4766_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4766)]\n",
    "asn_4766_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4766)]\n",
    "asn_4766_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4766)]\n",
    "asn_4766 = df.loc[df['r_asn'] == 4766]\n",
    "\n",
    "asn_4812_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4812)]\n",
    "asn_4812_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4812)]\n",
    "asn_4812_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4812)]\n",
    "asn_4812 = df.loc[df['r_asn'] == 4812]\n",
    "\n",
    "asn_4812_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4812)]\n",
    "asn_4812_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4812)]\n",
    "asn_4812_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4812)]\n",
    "asn_4812 = df.loc[df['r_asn'] == 4812]\n",
    "\n",
    "asn_4837_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 4837)]\n",
    "asn_4837_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 4837)]\n",
    "asn_4837_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 4837)]\n",
    "asn_4837 = df.loc[df['r_asn'] == 4837]\n",
    "\n",
    "\n",
    "asn_3561_c1 = df.loc[(df['l_ipn']== 0) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c3 = df.loc[(df['l_ipn']== 2) & (df['r_asn'] == 3561)]\n",
    "\n",
    "asn_3561_c4 = df.loc[(df['l_ipn']== 3) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c6 = df.loc[(df['l_ipn']== 5) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c7 = df.loc[(df['l_ipn']== 6) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c2 = df.loc[(df['l_ipn']== 1) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c5 = df.loc[(df['l_ipn']== 4) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c8 = df.loc[(df['l_ipn']== 7) & (df['r_asn'] == 3561)]\n",
    "asn_3561_c9 = df.loc[(df['l_ipn']== 8) & (df['r_asn'] == 3561)]\n",
    "asn_3561 = df.loc[df['r_asn'] == 3561]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc553fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 3269\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_3269['date'],asn_3269['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_3269_c4['date'],asn_3269_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_3269_c6['date'],asn_3269_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_3269_c7['date'],asn_3269_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 3269',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 3269\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_3320['date'],asn_3320['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_3320_c4['date'],asn_3320_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_3320_c6['date'],asn_3320_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_3320_c7['date'],asn_3320_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 3320',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 3462\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_4134['date'],asn_4134['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_4134_c4['date'],asn_4134_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_4134_c6['date'],asn_4134_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_4134_c7['date'],asn_4134_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 4134',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 4713\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_4713['date'],asn_4713['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_4713_c4['date'],asn_4713_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_4713_c6['date'],asn_4713_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_4713_c7['date'],asn_4713_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 4713',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f25fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 4766\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_4766['date'],asn_4766['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_4766_c4['date'],asn_4766_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_4766_c6['date'],asn_4766_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_4766_c7['date'],asn_4766_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 4766',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 4812\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_4812['date'],asn_4812['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_4812_c4['date'],asn_4812_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_4812_c6['date'],asn_4812_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_4812_c7['date'],asn_4812_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 4812',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 4837\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_4837['date'],asn_4837['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_4837_c4['date'],asn_4837_c4['f'], color= 'r', label='C4')\n",
    "plot.plot(asn_4837_c6['date'],asn_4837_c6['f'], color= 'g', label='C6')\n",
    "plot.plot(asn_4837_c7['date'],asn_4837_c7['f'], color= 'black', label='C7')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 4837',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fe2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Flows of Correlated Flows of server 3561\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(asn_3561['date'],asn_3561['f'], color= 'darkorange', label='ALL Computers')\n",
    "plot.plot(asn_3561_c1['date'],asn_3561_c1['f'], color= 'r', label='C1')\n",
    "plot.plot(asn_3561_c3['date'],asn_3561_c3['f'], color= 'black', label='C3')\n",
    "#plot.plot(asn_3561_c4['date'],asn_3561_c4['f'], color= 'lightseagreen', label='C4')\n",
    "#plot.plot(asn_3561_c6['date'],asn_3561_c6['f'], color= 'lime', label='C6')\n",
    "#plot.plot(asn_3561_c7['date'],asn_3561_c7['f'], color= 'turquoise', label='C7')\n",
    "plot.plot(asn_3561_c2['date'],asn_3561_c2['f'], color= 'fuchsia', label='C2')\n",
    "#plot.plot(asn_3561_c5['date'],asn_3561_c5['f'], color= 'darkgrey', label='C5')\n",
    "#plot.plot(asn_3561_c8['date'],asn_3561_c8['f'], color= 'darkgrey', label='C8')\n",
    "#plot.plot(asn_3561_c9['date'],asn_3561_c9['f'], color= 'darkgrey', label='C9')\n",
    "plot.xticks(rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Correlated Flows with server 3561',fontsize=30,ha='center')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853ab25",
   "metadata": {},
   "source": [
    "# _3 Calculating and Visualizing Aggregate Connection Sessions (number of connections established by l_ipn computers with r_asn each server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating: a variable for each computer to store their aggregate number of connection sessions for use in the time series graphs.\n",
    "Computer1 = df.loc[df[\"l_ipn\"] == 0]\n",
    "Computer2 = df.loc[df[\"l_ipn\"] == 1]\n",
    "Computer3 = df.loc[df[\"l_ipn\"] == 2]\n",
    "Computer4 = df.loc[df[\"l_ipn\"] == 3]\n",
    "Computer5 = df.loc[df[\"l_ipn\"] == 4]\n",
    "Computer6 = df.loc[df[\"l_ipn\"] == 5]\n",
    "Computer7 = df.loc[df[\"l_ipn\"] == 6]\n",
    "Computer8 = df.loc[df[\"l_ipn\"] == 7]\n",
    "Computer9 = df.loc[df[\"l_ipn\"] == 8]\n",
    "Computer10 = df.loc[df[\"l_ipn\"] == 9]\n",
    "\n",
    "daily_Serv_Count = df.groupby(['date'])[['r_asn']].count() \n",
    "daily_Serv_Count.reset_index(inplace = True)\n",
    "daily_Serv_Count\n",
    "\n",
    "daily_Serv_Count1 = Computer1.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count1.reset_index(inplace = True)\n",
    "daily_Serv_Count1 = daily_Serv_Count1.rename(columns={'r_asn':'Srv-to-Comp#1'})\n",
    "\n",
    "daily_Serv_Count2 = Computer2.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count2.reset_index(inplace = True)\n",
    "daily_Serv_Count2 = daily_Serv_Count2.rename(columns={'r_asn':'Srv-to-Comp#2'})\n",
    "\n",
    "daily_Serv_Count3 = Computer3.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count3.reset_index(inplace = True)\n",
    "daily_Serv_Count3 = daily_Serv_Count3.rename(columns={'r_asn':'Srv-to-Comp#3'})\n",
    "\n",
    "daily_Serv_Count4 = Computer4.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count4.reset_index(inplace = True)\n",
    "daily_Serv_Count4 = daily_Serv_Count4.rename(columns={'r_asn':'Srv-to-Comp#4'})\n",
    "\n",
    "daily_Serv_Count5 = Computer5.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count5.reset_index(inplace = True)\n",
    "daily_Serv_Count5 = daily_Serv_Count5.rename(columns={'r_asn':'Srv-to-Comp#5'})\n",
    "\n",
    "daily_Serv_Count6 = Computer6.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count6.reset_index(inplace = True)\n",
    "daily_Serv_Count6 = daily_Serv_Count6.rename(columns={'r_asn':'Srv-to-Comp#6'})\n",
    "\n",
    "daily_Serv_Count7 = Computer7.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count7.reset_index(inplace = True)\n",
    "daily_Serv_Count7 = daily_Serv_Count7.rename(columns={'r_asn':'Srv-to-Comp#7'})\n",
    "\n",
    "daily_Serv_Count8 = Computer10.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count8.reset_index(inplace = True)\n",
    "daily_Serv_Count8 = daily_Serv_Count8.rename(columns={'r_asn':'Srv-to-Comp#8'})\n",
    "\n",
    "daily_Serv_Count9 = Computer9.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count9.reset_index(inplace = True)\n",
    "daily_Serv_Count9 = daily_Serv_Count9.rename(columns={'r_asn':'Srv-to-Comp#9'})\n",
    "\n",
    "daily_Serv_Count10 = Computer10.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count10.reset_index(inplace = True)\n",
    "daily_Serv_Count10 = daily_Serv_Count10.rename(columns={'r_asn':'Srv-to-Comp#10'})\n",
    "\n",
    "daily_Serv_Count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46068a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a dictionary with the info posted with the data set challenge about suspected attack dates.\n",
    "dictionary = {'2006-08-24':1,'2006-09-04':5,'2006-09-18':4,'2006-09-26':3}\n",
    "marked_anom = pandas.DataFrame.from_dict(dictionary,orient='index')\n",
    "marked_anom.reset_index(inplace = True)\n",
    "marked_anom.columns = ['date','l_ipn']\n",
    "print(marked_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Daily Aggregate of # servers contacted by each of the ten computers.\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_Serv_Count['date'],daily_Serv_Count['r_asn'])\n",
    "plot.plot(daily_Serv_Count1['date'],daily_Serv_Count1['Srv-to-Comp#1'], color= 'r', label='C1')\n",
    "plot.plot(daily_Serv_Count2['date'],daily_Serv_Count2['Srv-to-Comp#2'], color= 'yellow', label='C2')\n",
    "plot.plot(daily_Serv_Count3['date'],daily_Serv_Count3['Srv-to-Comp#3'], color= 'black', label='C3')\n",
    "plot.plot(daily_Serv_Count4['date'],daily_Serv_Count4['Srv-to-Comp#4'], color= 'green', label='C4')\n",
    "plot.plot(daily_Serv_Count5['date'],daily_Serv_Count5['Srv-to-Comp#5'], color= 'springgreen', label='C5')\n",
    "plot.plot(daily_Serv_Count6['date'],daily_Serv_Count6['Srv-to-Comp#6'], color= 'cornflowerblue', label='C6')\n",
    "plot.plot(daily_Serv_Count7['date'],daily_Serv_Count7['Srv-to-Comp#7'], color= 'm', label='C7')\n",
    "plot.plot(daily_Serv_Count8['date'],daily_Serv_Count8['Srv-to-Comp#8'], color= 'darkviolet', label='C8')\n",
    "plot.plot(daily_Serv_Count9['date'],daily_Serv_Count9['Srv-to-Comp#9'], color= 'white', label='C9')\n",
    "plot.plot(daily_Serv_Count10['date'],daily_Serv_Count10['Srv-to-Comp#10'], color= 'aquamarine', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.xticks(daily_Serv_Count['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Daily Aggregate of # servers contacted during the 92 day observed period',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_Serv_Count['date'],daily_Serv_Count['r_asn'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a correlation matrix of all computer's aggregated connection sessions.\n",
    "corrMatrix = pandas.concat([daily_Serv_Count1['Srv-to-Comp#1'], daily_Serv_Count2['Srv-to-Comp#2'], daily_Serv_Count3['Srv-to-Comp#3'], daily_Serv_Count4['Srv-to-Comp#4'], daily_Serv_Count5['Srv-to-Comp#5'], daily_Serv_Count6['Srv-to-Comp#6'], daily_Serv_Count7['Srv-to-Comp#7'], daily_Serv_Count8['Srv-to-Comp#8'], daily_Serv_Count9['Srv-to-Comp#9'], daily_Serv_Count10['Srv-to-Comp#10']], axis=1).corr()\n",
    "plot.subplots(figsize=(10,10))\n",
    "seaborn.heatmap(corrMatrix.iloc[:, 0:10:], annot=True, linewidths=4)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd32a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating report of ALL daily aggregate connections sessions between each computer and servers.\n",
    "daily_aggreg_connections_ALL = pandas.DataFrame.from_dict(\n",
    "    {'Srv-to-Comp#1':daily_Serv_Count1['Srv-to-Comp#1'],\n",
    "     'Srv-to-Comp#2':daily_Serv_Count2['Srv-to-Comp#2'],\n",
    "     'Srv-to-Comp#3':daily_Serv_Count3['Srv-to-Comp#3'],\n",
    "     'Srv-to-Comp#4':daily_Serv_Count4['Srv-to-Comp#4'],\n",
    "     'Srv-to-Comp#5':daily_Serv_Count5['Srv-to-Comp#5'],\n",
    "     'Srv-to-Comp#6':daily_Serv_Count6['Srv-to-Comp#6'],\n",
    "     'Srv-to-Comp#7':daily_Serv_Count7['Srv-to-Comp#7'],\n",
    "     'Srv-to-Comp#8':daily_Serv_Count8['Srv-to-Comp#8'],\n",
    "     'Srv-to-Comp#9':daily_Serv_Count9['Srv-to-Comp#9'],\n",
    "     'Srv-to-Comp#10':daily_Serv_Count10['Srv-to-Comp#10']\n",
    "    }, orient='index')\n",
    "\n",
    "daily_aggreg_connections_ALL.transpose().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_aggregate = df.groupby(['date'])[['f']].sum() #Daily aggregate of all traffic(from previous page about flows).+ccoaily_aggregate.reset_index(inplace = True)\n",
    "\n",
    "#Plotting a correlation matrix of all daily flows vs. server connection sessions.\n",
    "corrMatrix = pandas.concat([daily_Serv_Count['r_asn'], daily_aggregate['f']], axis=1).corr()\n",
    "plot.subplots(figsize=(10,10))\n",
    "seaborn.heatmap(corrMatrix.iloc[:, 0:10:], cmap=\"Blues\", annot=True, annot_kws={\"size\": 12, \"va\": \"center_baseline\", \"color\": \"black\" },linewidths=4)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa13913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate report of daily number of flows by day, by computer (data Desription).\n",
    "daily_aggreg_flows_df = df.set_index(['date','r_asn','l_ipn']).unstack('l_ipn') \n",
    "daily_aggreg_flows_df.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate report of daily number of flows by day, by computer.\n",
    "daily_aggreg_flows_df.describe().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d32909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating: a variable for each computer to store their aggregate # of connections, for use in the time series graphs.\n",
    "Computer1 = df.loc[df[\"l_ipn\"] == 0]\n",
    "Computer2 = df.loc[df[\"l_ipn\"] == 1]\n",
    "Computer3 = df.loc[df[\"l_ipn\"] == 2]\n",
    "Computer4 = df.loc[df[\"l_ipn\"] == 3]\n",
    "Computer5 = df.loc[df[\"l_ipn\"] == 4]\n",
    "Computer6 = df.loc[df[\"l_ipn\"] == 5]\n",
    "Computer7 = df.loc[df[\"l_ipn\"] == 6]\n",
    "Computer8 = df.loc[df[\"l_ipn\"] == 7]\n",
    "Computer9 = df.loc[df[\"l_ipn\"] == 8]\n",
    "Computer10 = df.loc[df[\"l_ipn\"] == 9]\n",
    "\n",
    "daily_Serv_Count = df.groupby(['date'])[['r_asn']].count() \n",
    "daily_Serv_Count.reset_index(inplace = True)\n",
    "daily_Serv_Count\n",
    "\n",
    "daily_Serv_Count1 = Computer1.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count1.reset_index(inplace = True)\n",
    "daily_Serv_Count1 = daily_Serv_Count1.rename(columns={'r_asn':'Srv-to-Comp#1'})\n",
    "\n",
    "daily_Serv_Count2 = Computer2.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count2.reset_index(inplace = True)\n",
    "daily_Serv_Count2 = daily_Serv_Count2.rename(columns={'r_asn':'Srv-to-Comp#2'})\n",
    "\n",
    "daily_Serv_Count3 = Computer3.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count3.reset_index(inplace = True)\n",
    "daily_Serv_Count3 = daily_Serv_Count3.rename(columns={'r_asn':'Srv-to-Comp#3'})\n",
    "\n",
    "daily_Serv_Count4 = Computer4.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count4.reset_index(inplace = True)\n",
    "daily_Serv_Count4 = daily_Serv_Count4.rename(columns={'r_asn':'Srv-to-Comp#4'})\n",
    "\n",
    "daily_Serv_Count5 = Computer5.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count5.reset_index(inplace = True)\n",
    "daily_Serv_Count5 = daily_Serv_Count5.rename(columns={'r_asn':'Srv-to-Comp#5'})\n",
    "\n",
    "daily_Serv_Count6 = Computer6.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count6.reset_index(inplace = True)\n",
    "daily_Serv_Count6 = daily_Serv_Count6.rename(columns={'r_asn':'Srv-to-Comp#6'})\n",
    "\n",
    "daily_Serv_Count7 = Computer7.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count7.reset_index(inplace = True)\n",
    "daily_Serv_Count7 = daily_Serv_Count7.rename(columns={'r_asn':'Srv-to-Comp#7'})\n",
    "\n",
    "daily_Serv_Count8 = Computer8.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count8.reset_index(inplace = True)\n",
    "daily_Serv_Count8 = daily_Serv_Count8.rename(columns={'r_asn':'Srv-to-Comp#8'})\n",
    "\n",
    "daily_Serv_Count9 = Computer9.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count9.reset_index(inplace = True)\n",
    "daily_Serv_Count9 = daily_Serv_Count9.rename(columns={'r_asn':'Srv-to-Comp#9'})\n",
    "\n",
    "daily_Serv_Count10 = Computer10.groupby(['date'])[['r_asn']].count()\n",
    "daily_Serv_Count10.reset_index(inplace = True)\n",
    "daily_Serv_Count10 = daily_Serv_Count10.rename(columns={'r_asn':'Srv-to-Comp#10'})\n",
    "\n",
    "daily_Serv_Count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a dictionary with the info posted with the data set challenge about suspected attack dates.\n",
    "dictionary = {'2006-08-24':1,'2006-09-04':5,'2006-09-18':4,'2006-09-26':3}\n",
    "marked_anom = pandas.DataFrame.from_dict(dictionary,orient='index')\n",
    "marked_anom.reset_index(inplace = True)\n",
    "marked_anom.columns = ['date','l_ipn']\n",
    "print(marked_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d302bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Daily Aggregate of # servers contacted by each of the ten computers.\n",
    "plot.figure(figsize=(30,12))\n",
    "plot.plot(daily_Serv_Count['date'],daily_Serv_Count['r_asn'])\n",
    "plot.plot(daily_Serv_Count1['date'],daily_Serv_Count1['Srv-to-Comp#1'], color= 'r', label='C1')\n",
    "plot.plot(daily_Serv_Count2['date'],daily_Serv_Count2['Srv-to-Comp#2'], color= 'yellow', label='C2')\n",
    "plot.plot(daily_Serv_Count3['date'],daily_Serv_Count3['Srv-to-Comp#3'], color= 'black', label='C3')\n",
    "plot.plot(daily_Serv_Count4['date'],daily_Serv_Count4['Srv-to-Comp#4'], color= 'green', label='C4')\n",
    "plot.plot(daily_Serv_Count5['date'],daily_Serv_Count5['Srv-to-Comp#5'], color= 'springgreen', label='C5')\n",
    "plot.plot(daily_Serv_Count6['date'],daily_Serv_Count6['Srv-to-Comp#6'], color= 'cornflowerblue', label='C6')\n",
    "plot.plot(daily_Serv_Count7['date'],daily_Serv_Count7['Srv-to-Comp#7'], color= 'm', label='C7')\n",
    "plot.plot(daily_Serv_Count8['date'],daily_Serv_Count8['Srv-to-Comp#8'], color= 'darkviolet', label='C8')\n",
    "plot.plot(daily_Serv_Count9['date'],daily_Serv_Count9['Srv-to-Comp#9'], color= 'white', label='C9')\n",
    "plot.plot(daily_Serv_Count10['date'],daily_Serv_Count10['Srv-to-Comp#10'], color= 'aquamarine', label='C10')\n",
    "[plot.axvline(x=_x, color='r') for _x,ip in list(marked_anom[['date','l_ipn']].to_records(index=False))]\n",
    "plot.xticks(daily_Serv_Count['date'][::1],  rotation='vertical')\n",
    "plot.yscale('log')\n",
    "plot.xlabel('date')\n",
    "plot.ylabel('Connection')\n",
    "plot.title('Daily Aggregate of # servers contacted during the 92 day observed period',fontsize=30,ha='center')\n",
    "plot.fill_between(daily_Serv_Count['date'],daily_Serv_Count['r_asn'],color='darkgray')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b358072",
   "metadata": {},
   "source": [
    "# _4 Applying Markov Chains Prediction Method using the computers with the healthiest connections and flow activity as baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe with all daily aggregate connection sessions between SAFE computers and servers.\n",
    "daily_aggreg_connections_SAFE = pandas.DataFrame.from_dict(\n",
    "    {\n",
    "     #'Srv-to-Comp#1':daily_Serv_Count1['Srv-to-Comp#1'],\n",
    "     #'Srv-to-Comp#2':daily_Serv_Count2['Srv-to-Comp#2'],\n",
    "     #'Srv-to-Comp#3':daily_Serv_Count3['Srv-to-Comp#3'],\n",
    "     #'Srv-to-Comp#4':daily_Serv_Count4['Srv-to-Comp#4'],\n",
    "     'Srv-to-Comp#5':daily_Serv_Count5['Srv-to-Comp#5'],\n",
    "     #'Srv-to-Comp#6':daily_Serv_Count6['Srv-to-Comp#6'],\n",
    "     #'Srv-to-Comp#7':daily_Serv_Count7['Srv-to-Comp#7'],\n",
    "     'Srv-to-Comp#8':daily_Serv_Count8['Srv-to-Comp#8'],\n",
    "     #'Srv-to-Comp#9':daily_Serv_Count9['Srv-to-Comp#9'],\n",
    "     'Srv-to-Comp#10':daily_Serv_Count10['Srv-to-Comp#10']\n",
    "    }, orient='index')\n",
    "\n",
    "daily_aggreg_connections_SAFE = daily_aggreg_connections_SAFE.transpose().fillna(0).astype(int)\n",
    "daily_aggreg_connections_SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe with all aggregate healthy computer's flow, which will be my healthy traffic baseline.\n",
    "#Then transforming the dataframe's columns into a list by adding the flow of all its rows.\n",
    "daily_agg_flow_SAFE = pandas.DataFrame.from_dict({\n",
    "    'flows_C#5':daily_aggregate_flow5['flows_C#5'],\n",
    "    'flows_C#8':daily_aggregate_flow8['flows_C#8'], \n",
    "    'flows_C#10':daily_aggregate_flow10['flows_C#10']\n",
    "    })\n",
    "daily_agg_flow_SAFE = daily_agg_flow_SAFE.fillna(0).astype(int)\n",
    "daily_agg_flow_SAFE_total_list = daily_agg_flow_SAFE['flows_C#5'] + daily_agg_flow_SAFE['flows_C#8'] + daily_agg_flow_SAFE['flows_C#10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging my flows dataframe with my new list of healthy flow totals into one dataframe to verify the results of the sum operation:\n",
    "daily_agg_flow_SAFE[\"safe_Outcome\"] = daily_agg_flow_SAFE_total_list\n",
    "daily_agg_flow_SAFE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1260b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging my list of healthy flow totals with the dataframe with the number of healthy computer connections.\n",
    "daily_aggreg_connections_SAFE['Safe_Outcome'] = daily_agg_flow_SAFE_total_list\n",
    "daily_aggreg_connections_SAFE_outcome = daily_aggreg_connections_SAFE\n",
    "daily_aggreg_connections_SAFE_outcome.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing all values with rates of change for each day.\n",
    "daily_aggreg_connections_SAFE_outcome_pct = daily_aggreg_connections_SAFE_outcome.pct_change()\n",
    "daily_aggreg_connections_SAFE_outcome_pct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning(qcut): create two labels to classify increase or decrease in the aggregate connections sessions of SAFE conputers\n",
    "# Binning: simplifies the data by classifying values into two categories. Then, merging all number of connections of healthy computers to create the Event Pattern string. \n",
    "# Srv-to-Comp#1\n",
    "#daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#1_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#1\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#3\n",
    "#daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#3_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#3\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#5\n",
    "daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#5_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#5\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#8\n",
    "daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#8_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#8\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#9\n",
    "#daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#9_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#9\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#10\n",
    "daily_aggreg_connections_SAFE_outcome_pct['Srv_to_Comp#10_DU'] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Srv-to-Comp#10\"], 2, labels=[\"D\",\"U\"])\n",
    "# Srv-to-Comp#10\n",
    "daily_aggreg_connections_SAFE_outcome_pct[\"Outcome_Pct_DU\"] = pandas.qcut(daily_aggreg_connections_SAFE_outcome_pct[\"Safe_Outcome\"], 2, labels=[\"D\",\"U\"])                                                               \n",
    "# new set\n",
    "daily_aggreg_connections_SAFE_outcome_pct = daily_aggreg_connections_SAFE_outcome_pct[[\n",
    "                         #\"Srv_to_Comp#1_DU\", \n",
    "                         #\"Srv_to_Comp#3_DU\", \n",
    "                         \"Srv_to_Comp#5_DU\", \n",
    "                         \"Srv_to_Comp#8_DU\", \n",
    "                         #\"Srv_to_Comp#9_DU\",\n",
    "                         \"Srv_to_Comp#10_DU\",\n",
    "                         \"Outcome_Pct_DU\"]]  \n",
    "\n",
    "#Report: removing NAN values\n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN = daily_aggreg_connections_SAFE_outcome_pct.dropna()  \n",
    "#daily_aggreg_connections_SAFE_outcome_pct_NoNaN.reset_index(inplace=True)\n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN.head(5)\n",
    "\n",
    "#This line will correct \"copy df slice error on the next cel\"\n",
    "#pandas.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all number of connections of healthy computers to create the     Event Pattern string. \n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN[\"Event_Pattern_SAFE_Computers\"] = daily_aggreg_connections_SAFE_outcome_pct[\"Srv_to_Comp#5_DU\"].astype(str) + daily_aggreg_connections_SAFE_outcome_pct[\"Srv_to_Comp#8_DU\"].astype(str) +  daily_aggreg_connections_SAFE_outcome_pct[\"Srv_to_Comp#10_DU\"].astype(str)\n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac19c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compressing all flows letters.\n",
    "#daily_aggreg_connections_SAFE_outcome_pct_NoNaN.reset_index(inplace=True).\n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd = pandas.concat((daily_aggreg_connections_SAFE_outcome_pct_NoNaN[\"Event_Pattern_SAFE_Computers\"], daily_aggreg_connections_SAFE_outcome_pct_NoNaN[\"Outcome_Pct_DU\"]), axis=1)\n",
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5adfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd.describe() #Describe compressed list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e25117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate all UP outcome into two new dataframes.\n",
    "U_df = daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd[daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd['Outcome_Pct_DU']=='U']\n",
    "U_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate all DOWN outcome into two new dataframes.\n",
    "D_df = daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd[daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd['Outcome_Pct_DU']=='D']\n",
    "D_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing U with 1 and D with 0 in U_df.\n",
    "oneORzero = {'U': 1,'D': 0}\n",
    "U_df.Outcome_Pct_DU = [oneORzero[item] for item in U_df.Outcome_Pct_DU]\n",
    "U_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing U with 1 and D with 0 in D_df.\n",
    "oneORzero = {'U': 1,'D': 0}\n",
    "D_df.Outcome_Pct_DU = [oneORzero[item] for item in D_df.Outcome_Pct_DU]\n",
    "D_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fae1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding unique patterns.\n",
    "flat_list = [item.split(',') for item in daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd['Event_Pattern_SAFE_Computers'].values ]\n",
    "unique_patterns = ','.join(str(r) for v in flat_list for r in v)\n",
    "unique_patterns = list(set(unique_patterns.split(',')))\n",
    "unique_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34945761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the markov transition grid.\n",
    "def build_transition_grid(compressed_grid, unique_patterns):\n",
    "    # build the markov transition grid.\n",
    "    patterns = []\n",
    "    counts = []\n",
    "    for from_event in unique_patterns:\n",
    "        # how many times.\n",
    "        for to_event in unique_patterns:\n",
    "            pattern2 = from_event + ',' + to_event # DUD,DDU.\n",
    "            pattern1 = from_event\n",
    "            ids_matches =  compressed_grid[compressed_grid['Event_Pattern_SAFE_Computers'].str.contains(pattern1)]\n",
    "            found = 0\n",
    "            if len(ids_matches) > 0:\n",
    "                Event_Pattern_SAFE_Computers = '---'.join(ids_matches['Event_Pattern_SAFE_Computers'].values)\n",
    "                found = Event_Pattern_SAFE_Computers.count(pattern1)\n",
    "            patterns.append(pattern2)\n",
    "            counts.append(found)\n",
    "\n",
    "    # create to/from grid.\n",
    "    grid_Df = pandas.DataFrame({'pairs':patterns, 'counts': counts})\n",
    "\n",
    "    grid_Df['x'], grid_Df['y'] = grid_Df['pairs'].str.split(',', 1).str\n",
    "    grid_Df.head()\n",
    "\n",
    "    grid_Df = grid_Df.pivot(index='x', columns='y', values='counts')\n",
    "\n",
    "    grid_Df.columns=[col for col in grid_Df.columns]\n",
    "\n",
    "    # replace all NaN with zeros.\n",
    "    grid_Df.fillna(0, inplace=True)\n",
    "    grid_Df.head()\n",
    "\n",
    "    #Transition_dataframe.\n",
    "    grid_Df = grid_Df / grid_Df.sum(1)\n",
    "    return (grid_Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae30cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the matrix of healthy computer's connection increaase and decrease that lead to a flow increase.\n",
    "grid_UPs = build_transition_grid(daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd, unique_patterns) \n",
    "grid_UPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the matrix of healthy computer's connection increaase and decrease that lead to a flow decrease.\n",
    "grid_DOWNs = build_transition_grid(daily_aggreg_connections_SAFE_outcome_pct_NoNaN_cmprsd, unique_patterns) \n",
    "grid_DOWNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dc1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaf29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
